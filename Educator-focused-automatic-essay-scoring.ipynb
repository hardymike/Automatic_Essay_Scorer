{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, re, io\n",
    "import tensorflow as tf\n",
    "import itertools, collections\n",
    "import pickle\n",
    "import graphviz\n",
    "import pydot\n",
    "import spacy\n",
    "from time import time\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from csv import reader\n",
    "## Tensorflow Models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow_addons.utils.types import Number\n",
    "from typeguard import typechecked\n",
    "from typing import Optional\n",
    "## Evaluation libraries\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional packages for USE\n",
    "#@title Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "import pysbd\n",
    "from pysbd.utils import PySBDFactory\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants and Paths\n",
    "\n",
    "data_path = './'  # path to ner_dataset.csv file , from\n",
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + \\\n",
    "    '/home/hardy_mike/anaconda3/lib/python3.7/site-packages'\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "now = datetime.datetime.now()  # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0, data_path)\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'),\n",
    "                sep='\\t',\n",
    "                encoding='ISO-8859-1')\n",
    "X.index = X.essay_id\n",
    "\n",
    "p1 = pd.read_csv(os.path.join(DATASET_DIR, 'Prompt-1.csv'),\n",
    "                 index_col=\"EssayID\")\n",
    "p2 = pd.read_csv(os.path.join(DATASET_DIR, 'Prompt-2.csv'),\n",
    "                 index_col=\"Essay ID\")\n",
    "p3 = pd.read_csv(os.path.join(DATASET_DIR, 'Prompt-3.csv'),\n",
    "                 index_col=\"Essay ID\")\n",
    "p4 = pd.read_csv(os.path.join(DATASET_DIR, 'Prompt-4.csv'),\n",
    "                 index_col=\"Essay ID\")\n",
    "p5 = pd.read_csv(os.path.join(DATASET_DIR, 'Prompt-5.csv'),\n",
    "                 index_col=\"Essay ID\")\n",
    "p6 = pd.read_csv(os.path.join(DATASET_DIR, 'Prompt-6.csv'),\n",
    "                 index_col=\"Essay ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load USE Embeddings (if already processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = pickle.load(open('embeds_large.pkl', 'rb'))\n",
    "X['embeds'] = embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Parsing and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'),\n",
    "                sep='\\t',\n",
    "                encoding='ISO-8859-1')\n",
    "X.index = X.essay_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Universal Sentence Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"  #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "print(\"module %s loaded\" % module_url)\n",
    "\n",
    "\n",
    "def embed(input):\n",
    "    return model(input)\n",
    "\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe(PySBDFactory(nlp))\n",
    "essay_sentences = [\n",
    "    embed(s.text) for s in [list(nlp(X.essay[i]).sents) for i in X.essay_id]\n",
    "]\n",
    "X['essay_sentences'] = essay_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = len(X)\n",
    "USE_embeds = np.array([\n",
    "    embed([s.text for s in list(nlp(X[:curr].essay[i]).sents)])\n",
    "    for i in X[:curr].essay_id\n",
    "])\n",
    "\n",
    "## Save USE embeddings for computation efficiency during experimentation\n",
    "pickle.dump(USE_embeds, open('embeds_large.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentlength = [len(e) for e in embeds]\n",
    "max_len = max(sentlength) + 1\n",
    "MAX_LEN = max_len\n",
    "\n",
    "AVG_LEN = np.mean(sentlength)\n",
    "embeddings = np.zeros((len(X), max_len, 512))\n",
    "masks = np.full((len(X), max_len, 512), -1e9)\n",
    "masks = np.zeros((len(X), max_len, 512))\n",
    "mul_masks = np.ones((len(X), max_len, 512))\n",
    "\n",
    "for i, e in enumerate(embeds):\n",
    "    embeddings[i] = 1\n",
    "    masks[i] = 1\n",
    "    mul_masks[i] = 0\n",
    "    for j, s in enumerate(e):\n",
    "        embeddings[i][j + 1] = s\n",
    "        masks[i][j + 1] = 1\n",
    "        mul_masks[i][j + 1] = 0\n",
    "\n",
    "masks_bool = tf.cast(masks, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Reference Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = X.copy(deep=True)\n",
    "FOLDS = 5\n",
    "EMBED_DIM = 512\n",
    "min_num_classes = 4\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True)\n",
    "curr = 512\n",
    "count = 1\n",
    "selected_essay_sets = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "avg_class_est = 11\n",
    "ytests = []\n",
    "ypreds = []\n",
    "kappa_table = []\n",
    "kappas = []\n",
    "result_history = []\n",
    "avg_len_dict = {1: 350, 2: 350, 3: 150, 4: 150, 5: 150, 6: 150, 7: 250, 8: 650}\n",
    "selected_essay_sets = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "set_reduction = 1\n",
    "pieces = set_reduction\n",
    "avg_len_dict = {1: 350, 2: 350, 3: 150, 4: 150, 5: 150, 6: 150, 7: 250, 8: 650}\n",
    "set_size_dict = {\n",
    "    1: 1785,\n",
    "    2: 1800,\n",
    "    3: 1726,\n",
    "    4: 1772,\n",
    "    5: 1805,\n",
    "    6: 1800,\n",
    "    7: 1730,\n",
    "    8: 918\n",
    "}\n",
    "max_set_size = 1805\n",
    "avg_ratio_dict = {\n",
    "    1: np.round(650 / 350),\n",
    "    2: np.round(650 / 350),\n",
    "    3: np.round(650 / 150),\n",
    "    4: np.round(650 / 150),\n",
    "    5: np.round(650 / 150),\n",
    "    6: np.round(650 / 150),\n",
    "    7: np.round(650 / 250),\n",
    "    8: np.round(650 / 650)\n",
    "}\n",
    "num_heads = {1: 8, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 8, 8: 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Ordinal Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKappaLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Implements a custom Weighted Kappa loss function with an \n",
    "    label smoothing and weightage options. Adapted from \n",
    "    tensorflow addons kappa_loss.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            weightage: Optional[str] = \"quadratic\",\n",
    "            name: Optional[str] = \"cohen_kappa_loss\",\n",
    "            epsilon: Optional[Number] = 1e-6,\n",
    "            dtype: Optional[tf.DType] = tf.float32,\n",
    "            reduction: str = tf.keras.losses.Reduction.NONE,\n",
    "            label_smoothing=0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(name=name, reduction=reduction)\n",
    "        if weightage not in (\"linear\", \"quadratic\"):\n",
    "            raise ValueError(\"Unknown kappa weighting type.\")\n",
    "        if label_smoothing >= 1 or label_smoothing < 0:\n",
    "            raise ValueError(\"Label smoothing must be from 0 to 1.\")\n",
    "        self.weightage = weightage\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.dtype = dtype\n",
    "        label_vec = tf.range(num_classes, dtype=dtype)\n",
    "        self.row_label_vec = tf.reshape(label_vec, [1, num_classes])\n",
    "        self.col_label_vec = tf.reshape(label_vec, [num_classes, 1])\n",
    "        col_mat = tf.tile(self.col_label_vec, [1, num_classes])\n",
    "        row_mat = tf.tile(self.row_label_vec, [num_classes, 1])\n",
    "        self.smooth = tf.cast(label_smoothing, dtype=self.dtype)\n",
    "        if weightage == \"linear\":\n",
    "            self.weight_mat = tf.abs(col_mat - row_mat)\n",
    "        else:\n",
    "            self.weight_mat = (col_mat - row_mat)**2\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32),\n",
    "                            depth=self.num_classes)[:, 0, :]\n",
    "        y_true = y_true * (1 - self.smooth) + self.smooth / 2\n",
    "        y_true = tf.cast(y_true, dtype=self.dtype)\n",
    "        batch_size = tf.shape(y_true)[0]\n",
    "        cat_labels = tf.matmul(y_true, self.col_label_vec)\n",
    "        cat_label_mat = tf.tile(cat_labels, [1, self.num_classes])\n",
    "        row_label_mat = tf.tile(self.row_label_vec, [batch_size, 1])\n",
    "        if self.weightage == \"linear\":\n",
    "            weight = tf.abs(cat_label_mat - row_label_mat)\n",
    "        else:\n",
    "            weight = (cat_label_mat - row_label_mat)**2\n",
    "        numerator = tf.reduce_sum(weight * y_pred)\n",
    "        label_dist = tf.reduce_sum(y_true, axis=0, keepdims=True)\n",
    "        pred_dist = tf.reduce_sum(y_pred, axis=0, keepdims=True)\n",
    "        w_pred_dist = tf.matmul(self.weight_mat, pred_dist, transpose_b=True)\n",
    "        denominator = tf.reduce_sum(tf.matmul(label_dist, w_pred_dist))\n",
    "        denominator /= tf.cast(batch_size, dtype=self.dtype)\n",
    "        loss = tf.math.divide_no_nan(numerator, denominator)\n",
    "        return loss + self.epsilon\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"weightage\": self.weightage,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"dtype\": self.dtype,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention for Transformers\n",
    "Code adapted from https://www.tensorflow.org/tutorials/text/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, :]\n",
    "\n",
    "\n",
    "#     return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k,\n",
    "                          transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "\n",
    "        mask = tf.matmul(mask, mask,\n",
    "                         transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        mask = create_padding_mask(mask)\n",
    "\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits,\n",
    "                                      axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            d_model, kernel_initializer=tf.keras.initializers.he_uniform())\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        seq_len = tf.shape(q)[1]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(\n",
    "            q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(\n",
    "            k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(\n",
    "            v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention,\n",
    "            perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention,\n",
    "            (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(\n",
    "            concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff,\n",
    "                              activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 warmup_steps=4000,\n",
    "                 factor=1,\n",
    "                 num_classes=10,\n",
    "                 is_sine=False):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.factor = factor\n",
    "        self.is_sine = is_sine\n",
    "        self.num_classes = num_classes\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        if self.is_sine:\n",
    "            return 0.005 + 0.004 * tf.math.sin(0.1 * (step - 3.14))\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(\n",
    "            arg1, arg2) * (self.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "\n",
    "for ess_set in selected_essay_sets:\n",
    "    count = 1\n",
    "    ess_history = []\n",
    "    results = []\n",
    "    class_results = []\n",
    "    regress_results = []\n",
    "    comb_results = []\n",
    "    ## Create the new subsets of data\n",
    "    current_span = round(len(X0[X0.essay_set == ess_set]) // pieces)\n",
    "\n",
    "    X = X0[X0.essay_set == ess_set][:current_span]\n",
    "    embs = embeddings[X0.essay_set == ess_set][:current_span]\n",
    "    msks = masks[X0.essay_set == ess_set][:current_span]\n",
    "    mms = mul_masks[X0.essay_set == ess_set][:current_span]\n",
    "    sent_masks = 1 - tf.math.reduce_max(msks, axis=-1)\n",
    "    sent_masks = sent_masks.numpy()\n",
    "    sent_masks = tf.math.reduce_max(msks,\n",
    "                                    axis=-1).numpy().astype(dtype=np.float32)\n",
    "    sent_masks = np.max(msks, axis=-1).astype(dtype=np.float32)\n",
    "\n",
    "    set_size = len(X)\n",
    "    test_size = set_size // 6\n",
    "\n",
    "    indices = np.random.permutation(np.arange(set_size))\n",
    "    dev_indices, test_indices = indices[test_size:], indices[:test_size]\n",
    "\n",
    "    ## Get the defining characteristics of the new data\n",
    "    classes = list(set(X.domain1_score))\n",
    "\n",
    "    max_class = max(classes)\n",
    "    min_class = min(classes)\n",
    "    num_classes = max_class - min_class + 1\n",
    "    labels = X.domain1_score - min_class\n",
    "\n",
    "    min_label = min(labels)\n",
    "    max_label = max(labels)\n",
    "\n",
    "    BATCH_SIZES = int(2**(np.ceil(np.log2(num_classes + 1)) + 2))\n",
    "    CURR_ESS_LEN = int(avg_len_dict[ess_set])\n",
    "    CURR_LEN_RATIO = avg_ratio_dict[ess_set]\n",
    "    SET_SIZE_RATIO = max_set_size / set_size_dict[ess_set]\n",
    "    HIDDEN_SIZE = int(2**np.ceil(np.log2(num_classes * 10)))\n",
    "    LENGTH_RATIO = int(EMBED_DIM // (2 * avg_ratio_dict[ess_set]))\n",
    "    EPOCHS = int(100 * np.sqrt(num_classes) +\n",
    "                 50) * 6  #min((150 + num_classes**2) // 2,\n",
    "    PATIENCE = int(min((num_classes) * 10, 285))\n",
    "    DROPOUT = min(0.65 + 0.01 * np.sqrt(num_classes), 0.75)\n",
    "    LEARNING_RATE = 0.0001 * num_classes + 0.01 * DROPOUT\n",
    "    LOSS_WEIGHT = 0.90 / (1 + np.exp(0.5 *\n",
    "                                     (num_classes - avg_class_est))) + 0.00075\n",
    "    NORM_CLIP = 2\n",
    "    NUM_HEADS = int(num_heads[ess_set])\n",
    "\n",
    "    ## Get transformed labels for regression\n",
    "    ys = np.array(labels)\n",
    "    ys_regression = ys / num_classes\n",
    "\n",
    "    ## Separate a test set\n",
    "    X_test_df = X.iloc[test_indices]\n",
    "    Xt = embs[test_indices]\n",
    "    Mt = msks[test_indices]\n",
    "    sMt = sent_masks[test_indices]\n",
    "    yt = ys[test_indices]\n",
    "    yt_regression = ys_regression[test_indices]\n",
    "\n",
    "    ## Reset the dev data\n",
    "    X = X.iloc[dev_indices]\n",
    "    embs = embs[dev_indices]\n",
    "    msks = msks[dev_indices]\n",
    "    sMs = sent_masks[dev_indices]\n",
    "    ys = ys[dev_indices]\n",
    "    ys_regression = ys_regression[dev_indices]\n",
    "\n",
    "    for traincv, testcv in cv.split(X):\n",
    "        print(\"=\" * 50 + \"Essay \" + str(ess_set) + \"=\" * 50)\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "\n",
    "        print(\"current number of classes = {}\".format(num_classes))\n",
    "        ## These are for 5-fold training sets\n",
    "        Xn = embs[traincv]\n",
    "        Mn = tf.cast(msks[traincv], dtype=tf.float32)  #, keepdims = True\n",
    "        sMn = sMs[traincv]\n",
    "        yn = ys[traincv]\n",
    "        yn_regression = ys_regression[traincv]\n",
    "\n",
    "        ## These are for 5-fold dev sets\n",
    "        Xv = embs[testcv]\n",
    "        Mv = tf.cast(msks[testcv], dtype=tf.float32)  #, keepdims = True\n",
    "        sMv = sMs[testcv]\n",
    "        yv = ys[testcv]\n",
    "        yv_regression = ys_regression[testcv]\n",
    "        model = tf.keras.models.Sequential([\n",
    "            layers.Masking(mask_value=0., input_shape=(86, 512)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LSTM(num_classes * 2,\n",
    "                        dropout=0.60,\n",
    "                        recurrent_dropout=0.10,\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=tf.keras.initializers.he_normal()),\n",
    "            layers.LayerNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(num_classes,\n",
    "                         activation='relu',\n",
    "                         bias_initializer=tf.keras.initializers.he_uniform(),\n",
    "                         kernel_initializer=tf.keras.initializers.he_normal()),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        loss = [\n",
    "            WeightedKappaLoss(num_classes=num_classes),\n",
    "            #                 tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE),\n",
    "        ]\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                    patience=PATIENCE,\n",
    "                                                    min_delta=0.05,\n",
    "                                                    restore_best_weights=True)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=[\n",
    "                'acc',\n",
    "                tf.keras.metrics.SparseCategoricalCrossentropy(), 'mae'\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        history = model.fit(Xn,\n",
    "                            yn,\n",
    "                            validation_data=(Xv, yv),\n",
    "                            batch_size=batch_sizes,\n",
    "                            epochs=EPOCHS,\n",
    "                            callbacks=[callback])\n",
    "        ess_history.append(history)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        y_pred = model.predict(Xt)\n",
    "        ypreds.append(y_pred)\n",
    "\n",
    "        y_pred = np.array([labels[i] for i in np.argmax(y_pred, axis=1)],\n",
    "                          dtype=np.int32)\n",
    "\n",
    "        print(confusion_matrix(yt, y_pred))\n",
    "        print(\"\\n--------Fold {} KAPPA--------\\n\".format(count))\n",
    "        print(cohen_kappa_score(yt, y_pred, weights='quadratic'))\n",
    "\n",
    "        model.evaluate(Xt, yt, batch_size=batch_sizes)\n",
    "        result = cohen_kappa_score(yt, y_pred, weights='quadratic')\n",
    "        print(\"Kappa Score: {}\".format(result))\n",
    "        results.append(result)\n",
    "        if count == FOLDS:\n",
    "            print(\"5__FOLD__KAPPA___SCORE___AVG: {}\".format(np.mean(results)))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    histories2.append(ess_history)\n",
    "    result_history.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories_mha_combined_objective_final = []\n",
    "\n",
    "# regression = True\n",
    "\n",
    "FOLDS = 5\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True)\n",
    "selected_essay_sets = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# selected_essay_sets = [3,8]\n",
    "num_classes = 0\n",
    "\n",
    "for ess_set in selected_essay_sets:\n",
    "    count = 1\n",
    "    ess_history = []\n",
    "    results = []\n",
    "    class_results = []\n",
    "    regress_results = []\n",
    "    comb_results = []\n",
    "    # Create the new subsets of data\n",
    "    current_span = round(len(X0[X0.essay_set == ess_set]) // 2.75)\n",
    "\n",
    "    X = X0[X0.essay_set == ess_set][:current_span]\n",
    "    embs = embeddings[X0.essay_set == ess_set][:current_span]\n",
    "    msks = masks[X0.essay_set == ess_set][:current_span]\n",
    "    mms = mul_masks[X0.essay_set == ess_set][:current_span]\n",
    "    sent_masks = 1 - tf.math.reduce_max(msks, axis=-1)\n",
    "    sent_masks = sent_masks.numpy()\n",
    "    sent_masks = tf.math.reduce_max(msks,\n",
    "                                    axis=-1).numpy().astype(dtype=np.float32)\n",
    "    sent_masks = np.max(msks, axis=-1).astype(dtype=np.float32)\n",
    "\n",
    "    set_size = len(X)\n",
    "    test_size = set_size // 6\n",
    "\n",
    "    indices = np.random.permutation(np.arange(set_size))\n",
    "    dev_indices, test_indices = indices[test_size:], indices[:test_size]\n",
    "\n",
    "    # Get the defining characteristics of the new data\n",
    "    classes = list(set(X.domain1_score))\n",
    "\n",
    "    max_class = max(classes)\n",
    "    min_class = min(classes)\n",
    "    num_classes = max_class - min_class + 1\n",
    "    labels = X.domain1_score - min_class\n",
    "\n",
    "    min_label = min(labels)\n",
    "    max_label = max(labels)\n",
    "    batch_sizes = int(2**np.ceil(np.log2(num_classes + 1)))\n",
    "\n",
    "    # Get transformed labels for regression\n",
    "    ys = np.array(labels)\n",
    "    ys_regression = ys / num_classes\n",
    "\n",
    "    # Separate a test set\n",
    "    X_test_df = X.iloc[test_indices]\n",
    "    Xt = embs[test_indices]\n",
    "    Mt = msks[test_indices]\n",
    "    sMt = sent_masks[test_indices]\n",
    "    yt = ys[test_indices]\n",
    "    yt_regression = ys_regression[test_indices]\n",
    "\n",
    "    # Reset the dev data\n",
    "    X = X.iloc[dev_indices]\n",
    "    embs = embs[dev_indices]\n",
    "    msks = msks[dev_indices]\n",
    "    sMs = sent_masks[dev_indices]\n",
    "    ys = ys[dev_indices]\n",
    "    ys_regression = ys_regression[dev_indices]\n",
    "\n",
    "    for traincv, testcv in cv.split(X):\n",
    "        print(\"=\" * 50 + \"Essay \" + str(ess_set) + \"=\" * 50)\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "\n",
    "        batch_sizes = int(2**np.ceil(np.log2(num_classes + 1)))\n",
    "\n",
    "        print(\"current number of classes = {}\".format(num_classes))\n",
    "        # These are for 5-fold training sets\n",
    "        Xn = embs[traincv]\n",
    "        Mn = tf.cast(msks[traincv], dtype=tf.float32)  # , keepdims = True\n",
    "        sMn = sMs[traincv]\n",
    "        yn = ys[traincv]\n",
    "        yn_regression = ys_regression[traincv]\n",
    "\n",
    "        # These are for 5-fold dev sets\n",
    "        Xv = embs[testcv]\n",
    "        Mv = tf.cast(msks[testcv], dtype=tf.float32)  # , keepdims = True\n",
    "        sMv = sMs[testcv]\n",
    "        yv = ys[testcv]\n",
    "        yv_regression = ys_regression[testcv]\n",
    "\n",
    "        Input_embeds = tf.keras.Input(shape=(\n",
    "            MAX_LEN,\n",
    "            EMBED_DIM,\n",
    "        ))\n",
    "        Input_masks = tf.keras.Input(shape=(\n",
    "            MAX_LEN,\n",
    "            EMBED_DIM,\n",
    "        ))\n",
    "        Input_sentence_masks = tf.keras.Input(shape=(MAX_LEN, ))\n",
    "\n",
    "        inputs = [Input_embeds, Input_masks, Input_sentence_masks]\n",
    "\n",
    "        q_input = layers.Dense(EMBED_DIM, name=\"Query_Input\")(inputs[0])\n",
    "        q_input = layers.Multiply()([q_input, inputs[1]])\n",
    "        v_input = layers.Dense(EMBED_DIM, name=\"Value_Input\")(inputs[0])\n",
    "        v_input = layers.Multiply()([v_input, inputs[1]])\n",
    "        k_input = layers.Dense(EMBED_DIM, name=\"Key_Input\")(inputs[0])\n",
    "        k_input = layers.Multiply()([k_input, inputs[1]])\n",
    "\n",
    "        mha_output1, attention_weights = MultiHeadAttention(d_model=EMBED_DIM,\n",
    "                                                            num_heads=8)(\n",
    "                                                                v=v_input,\n",
    "                                                                k=k_input,\n",
    "                                                                q=q_input,\n",
    "                                                                mask=None)\n",
    "        mha_masked = layers.Multiply(name=\"MHA_Masking_1\")(\n",
    "            [mha_output1, inputs[1]])\n",
    "        mha_masked = layers.Dropout(0.25)(mha_masked)\n",
    "        layer_norm_output = layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"MHA_Norm_1\")(mha_masked + mha_output1)\n",
    "        layer_norm_output = layers.Reshape(\n",
    "            target_shape=(MAX_LEN, EMBED_DIM))(layer_norm_output)\n",
    "        mha_output2, attention_weights = MultiHeadAttention(\n",
    "            d_model=EMBED_DIM, num_heads=8)(v=v_input,\n",
    "                                            k=k_input,\n",
    "                                            q=layer_norm_output,\n",
    "                                            mask=None)\n",
    "        #         mha_output2= layers.Multiply()([mha_output2, inputs[1]])\n",
    "        mha_output2 = layers.Dropout(0.25)(mha_output2)\n",
    "        layer_norm_output2 = layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"MHA_Norm_2\")(mha_output2 + layer_norm_output)\n",
    "        ffn_output = layers.Dense(EMBED_DIM, activation='relu',\n",
    "                                  name=\"DFF_1\")(layer_norm_output2)\n",
    "        ffn_output = layers.Dense(EMBED_DIM, name=\"DFF_2\")(ffn_output)\n",
    "        #         ffn_output= layers.Multiply()([ffn_output, inputs[1]])\n",
    "        ffn_output = layers.Dropout(0.1)(ffn_output)\n",
    "        layer_norm_output3 = layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"DFF_Norm\")(ffn_output + layer_norm_output2)\n",
    "        layer_norm_output3 = layers.Reshape(\n",
    "            target_shape=(MAX_LEN, EMBED_DIM))(layer_norm_output3)\n",
    "        layer_norm_output3 = layers.Multiply()([layer_norm_output3, inputs[1]])\n",
    "        layer_norm_output3 = layers.Masking(name=\"DFF_Mask\")(\n",
    "            layer_norm_output3, )\n",
    "        final_hidden = layers.Bidirectional(\n",
    "            layers.LSTM(EMBED_DIM // 2,\n",
    "                        dropout=0.40,\n",
    "                        recurrent_dropout=0,\n",
    "                        activation='tanh',\n",
    "                        use_bias=True,\n",
    "                        unroll=False,\n",
    "                        recurrent_activation='sigmoid'),\n",
    "            name=\"BiLSTM\")(inputs=layer_norm_output3)\n",
    "\n",
    "        outputs_class = layers.Dense(num_classes,\n",
    "                                     activation='softmax',\n",
    "                                     name=\"Classification\")(final_hidden)\n",
    "\n",
    "        outputs_regression = layers.Dense(1,\n",
    "                                          activation='sigmoid',\n",
    "                                          name=\"Regression\")(final_hidden)\n",
    "\n",
    "        final_outputs = [outputs_class, outputs_regression]\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=final_outputs)\n",
    "\n",
    "        loss = [\n",
    "            WeightedKappaLoss(num_classes=num_classes),\n",
    "            tf.keras.losses.MeanSquaredError(),\n",
    "        ]\n",
    "\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "        loss_p = 0.95 / (\n",
    "            1 + np.exp(num_classes - avg_class_est)\n",
    "        ) + 0.005  # (min_num_classes*0.98)**2/(num_classes**2)\n",
    "        model.compile(loss=loss,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['acc', 'mae'],\n",
    "                      loss_weights=[loss_p, (1 - loss_p) * num_classes])\n",
    "\n",
    "        callback = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='Classification_loss',\n",
    "                                             patience=(num_classes + 10) * 2,\n",
    "                                             min_delta=0.005,\n",
    "                                             restore_best_weights=True),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='Regression_loss',\n",
    "                                             patience=(num_classes + 10) * 2,\n",
    "                                             min_delta=0.0005,\n",
    "                                             restore_best_weights=True),\n",
    "        ]\n",
    "\n",
    "        history = model.fit([Xn, Mn, sMn], [yn, yn_regression],\n",
    "                            validation_data=([Xv, Mv,\n",
    "                                              sMv], [yv, yv_regression]),\n",
    "                            batch_size=512,\n",
    "                            epochs=100 + num_classes * 10,\n",
    "                            callbacks=[callback])\n",
    "        ess_history.append(history)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        y_pred = model.predict([Xt, Mt, sMt])\n",
    "\n",
    "        y_pred_class = y_pred[0]\n",
    "        y_pred_regression = y_pred[1]\n",
    "\n",
    "        y_pred_regression = np.round(y_pred_regression * num_classes)\n",
    "\n",
    "        y_pred_class = np.argmax(y_pred_class, axis=1)\n",
    "\n",
    "        y_comb = np.round(\n",
    "            np.add((y_pred_class * loss_p),\n",
    "                   (y_pred[1] * num_classes * (1 - loss_p))[:, 0]))\n",
    "\n",
    "        print(confusion_matrix(yt, y_pred_class))\n",
    "        print(\"\\n--------Fold {} Classification KAPPA--------\\n\".format(count))\n",
    "        class_result = cohen_kappa_score(yt, y_pred_class, weights='quadratic')\n",
    "        print(class_result)\n",
    "\n",
    "        print(confusion_matrix(yt, y_pred_regression))\n",
    "        print(\"\\n--------Fold {} Regression KAPPA--------\\n\".format(count))\n",
    "        #         model.evaluate(Xt,yt,batch_size = batch_sizes)\n",
    "        regress_result = cohen_kappa_score(yt,\n",
    "                                           y_pred_regression,\n",
    "                                           weights='quadratic')\n",
    "        print(regress_result)\n",
    "\n",
    "        print(confusion_matrix(yt, y_comb))\n",
    "        print(\"\\n--------Fold {} Combined KAPPA--------\\n\".format(count))\n",
    "        comb_result = cohen_kappa_score(yt, y_comb, weights='quadratic')\n",
    "        print(comb_result)\n",
    "\n",
    "        class_results.append(class_result)\n",
    "        regress_results.append(regress_result)\n",
    "        comb_results.append(comb_result)\n",
    "        if count == FOLDS:\n",
    "            print(\"{}__FOLD_CLASS_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(class_results)))\n",
    "            print(\"{}__FOLD_REGRESS_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(regress_results)))\n",
    "            print(\"{}__FOLD_COMB_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(comb_results)))\n",
    "        count += 1\n",
    "\n",
    "    histories_mha_combined_objective_final.append(ess_history)\n",
    "    result_history.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Multi-layer Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories_mha_combined_objective_final4 = []\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 warmup_steps=4000,\n",
    "                 factor=1,\n",
    "                 num_classes=10,\n",
    "                 is_sine=False):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.factor = factor\n",
    "        self.is_sine = is_sine\n",
    "        self.num_classes = num_classes\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        if self.is_sine:\n",
    "            return 0.005 + 0.004 * tf.math.sin(0.1 * (step - 3.14))\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(\n",
    "            arg1, arg2) * (self.factor)\n",
    "\n",
    "\n",
    "FOLDS = 5\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True)\n",
    "selected_essay_sets = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# selected_essay_sets = [7,8,1,2,4, 5, 6, 7,3]\n",
    "selected_essay_sets = [8]\n",
    "num_classes = 0\n",
    "pieces = 1.5\n",
    "avg_len_dict = {1: 350, 2: 350, 3: 150, 4: 150, 5: 150, 6: 150, 7: 250, 8: 650}\n",
    "set_size_dict = {\n",
    "    1: 1785,\n",
    "    2: 1800,\n",
    "    3: 1726,\n",
    "    4: 1772,\n",
    "    5: 1805,\n",
    "    6: 1800,\n",
    "    7: 1730,\n",
    "    8: 918\n",
    "}\n",
    "max_set_size = 1805\n",
    "avg_ratio_dict = {\n",
    "    1: np.round(650 / 350),\n",
    "    2: np.round(650 / 350),\n",
    "    3: np.round(650 / 150),\n",
    "    4: np.round(650 / 150),\n",
    "    5: np.round(650 / 150),\n",
    "    6: np.round(650 / 150),\n",
    "    7: np.round(650 / 250),\n",
    "    8: np.round(650 / 650)\n",
    "}\n",
    "num_heads = {1: 8, 2: 4, 3: 4, 4: 4, 5: 4, 6: 4, 7: 8, 8: 16}\n",
    "\n",
    "for ess_set in selected_essay_sets:\n",
    "    count = 1\n",
    "    essay_set_counter = 0\n",
    "    ess_history = []\n",
    "    results = []\n",
    "    class_results = []\n",
    "    regress_results = []\n",
    "    comb_results = []\n",
    "    # Create the new subsets of data\n",
    "    current_span = round(len(X0[X0.essay_set == ess_set]) // pieces)\n",
    "\n",
    "    X = X0[X0.essay_set == ess_set][:current_span]\n",
    "    embs = embeddings[X0.essay_set == ess_set][:current_span]\n",
    "    msks = masks[X0.essay_set == ess_set][:current_span]\n",
    "    mms = mul_masks[X0.essay_set == ess_set][:current_span]\n",
    "    sent_masks = 1 - tf.math.reduce_max(msks, axis=-1)\n",
    "    sent_masks = sent_masks.numpy()\n",
    "    sent_masks = tf.math.reduce_max(msks,\n",
    "                                    axis=-1).numpy().astype(dtype=np.float32)\n",
    "    sent_masks = np.max(msks, axis=-1).astype(dtype=np.float32)\n",
    "\n",
    "    set_size = len(X)\n",
    "    test_size = set_size // 6\n",
    "\n",
    "    indices = np.random.permutation(np.arange(set_size))\n",
    "    dev_indices, test_indices = indices[test_size:], indices[:test_size]\n",
    "\n",
    "    # Get the defining characteristics of the new data\n",
    "    classes = list(set(X.domain1_score))\n",
    "\n",
    "    max_class = max(classes)\n",
    "    min_class = min(classes)\n",
    "    num_classes = max_class - min_class + 1\n",
    "    labels = X.domain1_score - min_class\n",
    "\n",
    "    min_label = min(labels)\n",
    "    max_label = max(labels)\n",
    "\n",
    "    BATCH_SIZES = int(2**(np.ceil(np.log2(num_classes + 1)) + 2))\n",
    "    CURR_ESS_LEN = int(avg_len_dict[ess_set])\n",
    "    CURR_LEN_RATIO = avg_ratio_dict[ess_set]\n",
    "    SET_SIZE_RATIO = max_set_size / set_size_dict[ess_set]\n",
    "    HIDDEN_SIZE = int(2**np.ceil(np.log2(num_classes * 10)))\n",
    "    LENGTH_RATIO = int(EMBED_DIM // (2 * avg_ratio_dict[ess_set]))\n",
    "    EPOCHS = int(100 * np.sqrt(num_classes) +\n",
    "                 50) * 6  # min((150 + num_classes**2) // 2,\n",
    "    PATIENCE = int(min((num_classes) * 10, 285))\n",
    "    DROPOUT = min(0.65 + 0.01 * np.sqrt(num_classes), 0.75)\n",
    "    LEARNING_RATE = 0.0001 * num_classes + 0.01 * DROPOUT\n",
    "    LOSS_WEIGHT = 0.90 / (1 + np.exp(0.5 *\n",
    "                                     (num_classes - avg_class_est))) + 0.00075\n",
    "    NORM_CLIP = 2\n",
    "    NUM_HEADS = int(num_heads[ess_set])\n",
    "\n",
    "    # Get transformed labels for regression\n",
    "    ys = np.array(labels)\n",
    "    ys_regression = ys / num_classes\n",
    "\n",
    "    # Separate a test set\n",
    "    X_test_df = X.iloc[test_indices]\n",
    "    Xt = embs[test_indices]\n",
    "    Mt = msks[test_indices]\n",
    "    sMt = sent_masks[test_indices]\n",
    "    yt = ys[test_indices]\n",
    "    yt_regression = ys_regression[test_indices]\n",
    "\n",
    "    # Reset the dev data\n",
    "    X = X.iloc[dev_indices]\n",
    "    embs = embs[dev_indices]\n",
    "    msks = msks[dev_indices]\n",
    "    sMs = sent_masks[dev_indices]\n",
    "    ys = ys[dev_indices]\n",
    "    ys_regression = ys_regression[dev_indices]\n",
    "\n",
    "    for traincv, testcv in cv.split(X):\n",
    "        print(\"=\" * 50 + \"Essay \" + str(ess_set) + \"=\" * 50)\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "\n",
    "        print(\"current number of classes = {}\".format(num_classes))\n",
    "        # These are for 5-fold training sets\n",
    "        Xn = embs[traincv]\n",
    "        Mn = tf.cast(msks[traincv], dtype=tf.float32)  # , keepdims = True\n",
    "        sMn = sMs[traincv]\n",
    "        yn = ys[traincv]\n",
    "        yn_regression = ys_regression[traincv]\n",
    "\n",
    "        # These are for 5-fold dev sets\n",
    "        Xv = embs[testcv]\n",
    "        Mv = tf.cast(msks[testcv], dtype=tf.float32)  # , keepdims = True\n",
    "        sMv = sMs[testcv]\n",
    "        yv = ys[testcv]\n",
    "        yv_regression = ys_regression[testcv]\n",
    "\n",
    "        ### =======  Define Inputs ======= ###\n",
    "        Input_embeds = tf.keras.Input(shape=(\n",
    "            MAX_LEN,\n",
    "            EMBED_DIM,\n",
    "        ))\n",
    "        Input_masks = tf.keras.Input(shape=(\n",
    "            MAX_LEN,\n",
    "            EMBED_DIM,\n",
    "        ))\n",
    "        Input_sentence_masks = tf.keras.Input(shape=(MAX_LEN, ))\n",
    "        inputs = [Input_embeds, Input_masks, Input_sentence_masks]\n",
    "\n",
    "        ### =======  Define Query, Key, and Value for Transformer ======= ###\n",
    "        q_input = layers.Dense(EMBED_DIM, name=\"Query_Input\")(inputs[0])\n",
    "        v_input = layers.Dense(EMBED_DIM, name=\"Value_Input\")(inputs[0])\n",
    "        k_input = layers.Dense(EMBED_DIM, name=\"Key_Input\")(inputs[0])\n",
    "\n",
    "        ### =======  Masked MultiHead Attention ======= ###\n",
    "        mha_output1, attention_weights = MultiHeadAttention(\n",
    "            d_model=EMBED_DIM, num_heads=NUM_HEADS)(v=v_input,\n",
    "                                                    k=k_input,\n",
    "                                                    q=q_input,\n",
    "                                                    mask=inputs[0])\n",
    "\n",
    "        mha_masked = layers.Dropout(DROPOUT)(mha_output1)\n",
    "        forLabel = layers.Masking(name=\"DFF_Mask2\")(mha_masked, )\n",
    "\n",
    "        if num_classes > 1:\n",
    "            ### =======  Add and Norm ======= ###\n",
    "            layer_norm_output = layers.LayerNormalization(\n",
    "                epsilon=1e-6, name=\"MHA_Norm_1\")(q_input + mha_output1)\n",
    "\n",
    "            ### =======  Feed Forward Network ======= ###\n",
    "            ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                activation='relu',\n",
    "                name=\"ffn_2\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output)\n",
    "\n",
    "            ### =======  Add and Norm ======= ###\n",
    "            layer_norm_output2 = layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name=\"MHA_Norm_2\")(ffn_output + layer_norm_output)\n",
    "            q_ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                name=\"q_weight_2\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output)\n",
    "\n",
    "            v_ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                name=\"v_weight_2\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output)\n",
    "            k_ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                name=\"k_weight_2\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output)\n",
    "\n",
    "            ### =======  MultiHead Attention ======= ###\n",
    "            mha_output2, attention_weights = MultiHeadAttention(\n",
    "                d_model=EMBED_DIM, num_heads=NUM_HEADS)(v=v_ffn_output,\n",
    "                                                        k=k_ffn_output,\n",
    "                                                        q=q_ffn_output,\n",
    "                                                        mask=inputs[0])\n",
    "            mha_output2 = layers.Dropout(DROPOUT)(mha_output2)\n",
    "\n",
    "            ### =======  Add and Norm ======= ###\n",
    "            layer_norm_output2 = layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name=\"MHA_Norm_2\")(mha_output2 + layer_norm_output)\n",
    "\n",
    "            ### =======  Feed Forward Network ======= ###\n",
    "            ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                activation='relu',\n",
    "                name=\"DFF_1\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output2)\n",
    "            ffn_output = layers.Dropout(DROPOUT)(\n",
    "                ffn_output)  # 0.2 * SET_SIZE_RATIO\n",
    "            q_ffn = layers.Dense(EMBED_DIM,\n",
    "                                 name=\"Q_DFF_2\",\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                                     0.0001))(ffn_output)\n",
    "            q_ffn = layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name=\"Qffn_normalization\")(q_ffn + layer_norm_output2)\n",
    "\n",
    "            ### =======  MultiHead Attention PART DEUX ======= ###\n",
    "            mha_output3, attention_weights = MultiHeadAttention(\n",
    "                d_model=EMBED_DIM, num_heads=NUM_HEADS)(v=v_ffn_output,\n",
    "                                                        k=k_ffn_output,\n",
    "                                                        q=q_ffn,\n",
    "                                                        mask=inputs[0])\n",
    "            mha_output3 = layers.Dropout(DROPOUT)(mha_output3)\n",
    "\n",
    "            ### =======  Add and Norm ======= ###\n",
    "            layer_norm_output3 = layers.LayerNormalization(\n",
    "                epsilon=1e-6, name=\"MHA_Norm_3\")(mha_output3 + q_ffn)\n",
    "\n",
    "            ### =======  Feed Forward Network ======= ###\n",
    "            ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                activation='relu',\n",
    "                name=\"DFF_B\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output3)\n",
    "            ffn_output = layers.Dropout(DROPOUT)(\n",
    "                ffn_output)  # 0.2 * SET_SIZE_RATIO\n",
    "            q_ffn = layers.Dense(EMBED_DIM,\n",
    "                                 name=\"Q_DFF_B\",\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                                     0.0001))(ffn_output)\n",
    "            q_ffn = layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name=\"MHA_normalization_2\")(q_ffn + layer_norm_output3)\n",
    "\n",
    "            forLabel = q_ffn\n",
    "\n",
    "        cls_token = forLabel[:, 0, :]\n",
    "        outputs_combined_class = layers.Dense(num_classes,\n",
    "                                              activation='softmax',\n",
    "                                              name=\"Combined_Class\")(cls_token)\n",
    "        outputs_combined_reg = layers.Dense(1,\n",
    "                                            activation='sigmoid',\n",
    "                                            name=\"Combined_Regress\")(cls_token)\n",
    "\n",
    "        ### =======  Output ======= ###\n",
    "        final_outputs = [outputs_combined_class, outputs_combined_reg]\n",
    "\n",
    "        ### =======  Define Model Parameters and Compile ======= ###\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=final_outputs)\n",
    "\n",
    "        loss = [\n",
    "            WeightedKappaLoss(num_classes=num_classes, label_smoothing=0),\n",
    "            tf.keras.losses.MeanSquaredError(),\n",
    "            WeightedKappaLoss(num_classes=num_classes, label_smoothing=0),\n",
    "        ]\n",
    "\n",
    "        learning_rate = LEARNING_RATE\n",
    "        learning_rate = CustomSchedule(d_model=EMBED_DIM,\n",
    "                                       warmup_steps=750,\n",
    "                                       is_sine=False)\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['acc', 'mse'],\n",
    "            loss_weights=[LOSS_WEIGHT, 2 * num_classes * (1 - LOSS_WEIGHT)],\n",
    "        )\n",
    "\n",
    "        callback = [\n",
    "            #             tensorboard_callback,\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_Combined_Class_loss',\n",
    "                                             patience=PATIENCE * 2,\n",
    "                                             min_delta=0.004,\n",
    "                                             restore_best_weights=True),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_Combined_Regress_loss',\n",
    "                patience=PATIENCE * 4,\n",
    "                min_delta=0.00005,\n",
    "                restore_best_weights=True),\n",
    "        ]\n",
    "        history = model.fit(\n",
    "            [Xn, Mn, sMn],\n",
    "            [yn, yn_regression],\n",
    "            validation_data=([Xv, Mv, sMv], [yv, yv_regression]),\n",
    "            batch_size=512,  # BATCH_SIZES,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[callback])\n",
    "        ### =======  Record History and Summaries ======= ###\n",
    "        ess_history.append(history)\n",
    "        model.summary()\n",
    "\n",
    "        print(\"BATCH_SIZES = {}\".format(BATCH_SIZES))\n",
    "        print(\"HIDDEN_SIZE = {}\".format(HIDDEN_SIZE))\n",
    "        print(\"TOTAL EPOCHS =  {}\".format(EPOCHS))\n",
    "        print(\"PATIENCE = {}\".format(PATIENCE))\n",
    "        print(\"DROPOUT = {}\".format(DROPOUT))\n",
    "        print(\"LEARNING_RATE =  {}\".format(LEARNING_RATE))\n",
    "        print(\"percent classification = {} \".format(LOSS_WEIGHT))\n",
    "        print(\"NUM_HEADS = {}\".format(NUM_HEADS))\n",
    "\n",
    "        ### =======  Evaluate on Held-out Test Set ======= ###\n",
    "        y_pred = model.predict([Xt, Mt, sMt])\n",
    "        y_pred_class = y_pred[0]\n",
    "        y_pred_class_c = np.argmax(y_pred[0], axis=1)\n",
    "        y_pred_regression = y_pred[1]\n",
    "        y_pred_regression = np.round(y_pred_regression * num_classes)\n",
    "        y_pred_class = np.argmax(y_pred_class, axis=1)\n",
    "\n",
    "        print(confusion_matrix(yt, y_pred_class))\n",
    "        print(\"\\n--------Fold {} Classification KAPPA--------\\n\".format(count))\n",
    "        class_result = cohen_kappa_score(yt, y_pred_class, weights='quadratic')\n",
    "        print(class_result)\n",
    "        print(confusion_matrix(yt, y_pred_regression))\n",
    "        print(\"\\n--------Fold {} Regression KAPPA--------\\n\".format(count))\n",
    "        regress_result = cohen_kappa_score(yt,\n",
    "                                           y_pred_regression,\n",
    "                                           weights='quadratic')\n",
    "        print(regress_result)\n",
    "        y_comb = np.round(\n",
    "            np.add((np.argmax(y_pred[0], axis=1) * LOSS_WEIGHT),\n",
    "                   (y_pred[1] * num_classes * (1 - LOSS_WEIGHT))[:, 0]))\n",
    "\n",
    "        print(confusion_matrix(yt, y_comb))\n",
    "        print(\"\\n--------Fold {} Combined KAPPA--------\\n\".format(count))\n",
    "        comb_result = cohen_kappa_score(yt, y_comb, weights='quadratic')\n",
    "        print(comb_result)\n",
    "\n",
    "        class_results.append(class_result)\n",
    "        regress_results.append(regress_result)\n",
    "        comb_results.append(comb_result)\n",
    "        if count == FOLDS:\n",
    "            print(\"{}__FOLD_CLASS_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(class_results)))\n",
    "            print(\"{}__FOLD_REGRESS_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(regress_results)))\n",
    "            print(\"{}__FOLD_COMB_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(comb_results)))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    histories_mha_combined_objective_final4.append(ess_history)\n",
    "    result_history.append(results)\n",
    "    f, ax = plt.subplots(nrows=1, ncols=FOLDS, figsize=[15, 3], sharey='row')\n",
    "    for fd in range(FOLDS):\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['Combined_Regress_loss'])\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['val_Combined_Regress_loss'])\n",
    "    plt.title('model regress loss essay {}'.format(ess_set))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        'mha2_combined_regress_features_8_6_attn_model_length2_essay_{}.png'.\n",
    "        format(ess_set))\n",
    "\n",
    "    f, ax = plt.subplots(nrows=1, ncols=FOLDS, figsize=[15, 3], sharey='row')\n",
    "    for fd in range(FOLDS):\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['Combined_Class_loss'])\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['val_Combined_Class_loss'])\n",
    "    plt.title('model class loss essay {}'.format(ess_set))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        'mha2_combined_class_features_8_6_attn_model_length2_essay_{}.png'.\n",
    "        format(ess_set))\n",
    "    essay_set_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention with BLSTM processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories_mha_combined_objective_final4 = []\n",
    "\n",
    "for ess_set in selected_essay_sets:\n",
    "    count = 1\n",
    "    essay_set_counter = 0\n",
    "    ess_history = []\n",
    "    results = []\n",
    "    class_results = []\n",
    "    regress_results = []\n",
    "    comb_results = []\n",
    "    # Create the new subsets of data\n",
    "    current_span = round(len(X0[X0.essay_set == ess_set]) // pieces)\n",
    "\n",
    "    X = X0[X0.essay_set == ess_set][:current_span]\n",
    "    embs = embeddings[X0.essay_set == ess_set][:current_span]\n",
    "    msks = masks[X0.essay_set == ess_set][:current_span]\n",
    "    mms = mul_masks[X0.essay_set == ess_set][:current_span]\n",
    "    sent_masks = 1 - tf.math.reduce_max(msks, axis=-1)\n",
    "    sent_masks = sent_masks.numpy()\n",
    "    sent_masks = tf.math.reduce_max(msks,\n",
    "                                    axis=-1).numpy().astype(dtype=np.float32)\n",
    "    sent_masks = np.max(msks, axis=-1).astype(dtype=np.float32)\n",
    "\n",
    "    set_size = len(X)\n",
    "    test_size = set_size // 6\n",
    "\n",
    "    indices = np.random.permutation(np.arange(set_size))\n",
    "    dev_indices, test_indices = indices[test_size:], indices[:test_size]\n",
    "\n",
    "    # Get the defining characteristics of the new data\n",
    "    classes = list(set(X.domain1_score))\n",
    "    max_class = max(classes)\n",
    "    min_class = min(classes)\n",
    "    num_classes = max_class - min_class + 1\n",
    "    labels = X.domain1_score - min_class\n",
    "\n",
    "    min_label = min(labels)\n",
    "    max_label = max(labels)\n",
    "\n",
    "    BATCH_SIZES = int(2**(np.ceil(np.log2(num_classes + 1)) + 2))\n",
    "    CURR_ESS_LEN = int(avg_len_dict[ess_set])\n",
    "    CURR_LEN_RATIO = avg_ratio_dict[ess_set]\n",
    "    SET_SIZE_RATIO = max_set_size / set_size_dict[ess_set]\n",
    "    HIDDEN_SIZE = int(2**np.ceil(np.log2(num_classes * 10)))\n",
    "    LENGTH_RATIO = int(EMBED_DIM // (2 * avg_ratio_dict[ess_set]))\n",
    "    EPOCHS = int(100 * np.sqrt(num_classes) +\n",
    "                 50) * 6  # min((150 + num_classes**2) // 2,\n",
    "    PATIENCE = int(min((num_classes) * 10, 285))\n",
    "    DROPOUT = min(0.65 + 0.01 * np.sqrt(num_classes), 0.85)\n",
    "    LEARNING_RATE = 0.0001 * num_classes + 0.01 * DROPOUT\n",
    "    LOSS_WEIGHT = 0.90 / (1 + np.exp(0.5 *\n",
    "                                     (num_classes - avg_class_est))) + 0.00075\n",
    "    NORM_CLIP = 2\n",
    "    NUM_HEADS = int(num_heads[ess_set])\n",
    "\n",
    "    # Get transformed labels for regression\n",
    "    ys = np.array(labels)\n",
    "    ys_regression = ys / num_classes\n",
    "\n",
    "    # Separate a test set\n",
    "    X_test_df = X.iloc[test_indices]\n",
    "    Xt = embs[test_indices]\n",
    "    Mt = msks[test_indices]\n",
    "    sMt = sent_masks[test_indices]\n",
    "    yt = ys[test_indices]\n",
    "    yt_regression = ys_regression[test_indices]\n",
    "\n",
    "    # Reset the dev data\n",
    "    X = X.iloc[dev_indices]\n",
    "    embs = embs[dev_indices]\n",
    "    msks = msks[dev_indices]\n",
    "    sMs = sent_masks[dev_indices]\n",
    "    ys = ys[dev_indices]\n",
    "    ys_regression = ys_regression[dev_indices]\n",
    "\n",
    "    for traincv, testcv in cv.split(X):\n",
    "        print(\"=\" * 50 + \"Essay \" + str(ess_set) + \"=\" * 50)\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "\n",
    "        print(\"current number of classes = {}\".format(num_classes))\n",
    "        # These are for 5-fold training sets\n",
    "        Xn = embs[traincv]\n",
    "        Mn = tf.cast(msks[traincv], dtype=tf.float32)  # , keepdims = True\n",
    "        sMn = sMs[traincv]\n",
    "        yn = ys[traincv]\n",
    "        yn_regression = ys_regression[traincv]\n",
    "\n",
    "        # These are for 5-fold dev sets\n",
    "        Xv = embs[testcv]\n",
    "        Mv = tf.cast(msks[testcv], dtype=tf.float32)  # , keepdims = True\n",
    "        sMv = sMs[testcv]\n",
    "        yv = ys[testcv]\n",
    "        yv_regression = ys_regression[testcv]\n",
    "\n",
    "        ### =======  Define Inputs ======= ###\n",
    "        Input_embeds = tf.keras.Input(shape=(\n",
    "            MAX_LEN,\n",
    "            EMBED_DIM,\n",
    "        ))\n",
    "        Input_masks = tf.keras.Input(shape=(\n",
    "            MAX_LEN,\n",
    "            EMBED_DIM,\n",
    "        ))\n",
    "        Input_sentence_masks = tf.keras.Input(shape=(MAX_LEN, ))\n",
    "        inputs = [Input_embeds, Input_masks, Input_sentence_masks]\n",
    "\n",
    "        ### =======  Define Query, Key, and Value for Transformer ======= ###\n",
    "        q_input = layers.Dense(EMBED_DIM, name=\"Query_Input\")(inputs[0])\n",
    "\n",
    "        q_input = layers.Multiply()([q_input, inputs[1]])\n",
    "\n",
    "        v_input = layers.Dense(EMBED_DIM, name=\"Value_Input\")(inputs[0])\n",
    "\n",
    "        v_input = layers.Multiply()([v_input, inputs[1]])\n",
    "\n",
    "        if num_classes > 8:\n",
    "            k_input = layers.Dense(EMBED_DIM, name=\"Key_Input\")(inputs[0])\n",
    "\n",
    "            k_input = layers.Multiply()([k_input, inputs[1]])\n",
    "\n",
    "        else:\n",
    "            k_input = v_input\n",
    "\n",
    "        ### =======  Masked MultiHead Attention ======= ###\n",
    "        mha_output1, attention_weights = MultiHeadAttention(\n",
    "            d_model=EMBED_DIM, num_heads=NUM_HEADS)(v=v_input,\n",
    "                                                    k=k_input,\n",
    "                                                    q=q_input,\n",
    "                                                    mask=None)\n",
    "\n",
    "        mha_masked = layers.Multiply(name=\"MHA_Masking_1\")(\n",
    "            [mha_output1, inputs[1]])\n",
    "        mha_masked = layers.Dropout(DROPOUT)(mha_masked)\n",
    "        forLSTM = layers.Masking(name=\"DFF_Mask2\")(mha_masked, )\n",
    "\n",
    "        if num_classes > 5:\n",
    "            ### =======  Add and Norm ======= ###\n",
    "            layer_norm_output = layers.LayerNormalization(\n",
    "                epsilon=1e-6, name=\"MHA_Norm_1\")(mha_masked + mha_output1)\n",
    "\n",
    "            ### =======  Masked MultiHead Attention ======= ###\n",
    "            mha_output2, attention_weights = MultiHeadAttention(\n",
    "                d_model=EMBED_DIM, num_heads=NUM_HEADS)(v=v_input,\n",
    "                                                        k=k_input,\n",
    "                                                        q=layer_norm_output,\n",
    "                                                        mask=None)\n",
    "            mha_output2 = layers.Multiply()([mha_output2, inputs[1]])\n",
    "            mha_output2 = layers.Dropout(DROPOUT)(mha_output2)\n",
    "\n",
    "            ### =======  Add and Norm ======= ###\n",
    "            layer_norm_output2 = layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name=\"MHA_Norm_2\")(mha_output2 + layer_norm_output)\n",
    "\n",
    "            ### =======  Feed Forward Network ======= ###\n",
    "            ffn_output = layers.Dense(\n",
    "                EMBED_DIM,\n",
    "                activation='relu',\n",
    "                name=\"DFF_1\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                    layer_norm_output2)\n",
    "            ffn_output = layers.Dropout(DROPOUT)(\n",
    "                ffn_output)  # 0.2 * SET_SIZE_RATIO\n",
    "            q_ffn = layers.Dense(EMBED_DIM,\n",
    "                                 name=\"Q_DFF_2\",\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                                     0.0001))(ffn_output)\n",
    "            q_ffn = layers.LayerNormalization(epsilon=1e-6,\n",
    "                                              name=\"Qffn_normalization\")(q_ffn)\n",
    "            q_ffn = layers.Multiply()([q_ffn, inputs[1]])\n",
    "            forLSTM = layers.Masking(name=\"DFF_Mask2\")(q_ffn, )\n",
    "\n",
    "            if num_classes > 100:\n",
    "                ### =======  Feed Forward Network for Second Layer ======= ###\n",
    "                DROPOUT_2 = 0.5\n",
    "                v_ffn = layers.Dense(\n",
    "                    EMBED_DIM,\n",
    "                    name=\"V_DFF_2\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.0001))(\n",
    "                        ffn_output)\n",
    "                v_ffn = layers.Multiply()([v_ffn, inputs[1]])\n",
    "                v_ffn = layers.LayerNormalization(\n",
    "                    epsilon=1e-6, name=\"Vffn_normalization\")(v_ffn)\n",
    "                v_ffn = layers.Multiply()([v_ffn, inputs[1]])\n",
    "                k_ffn = layers.Dense(\n",
    "                    EMBED_DIM,\n",
    "                    name=\"K_DFF_2\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.0001))(\n",
    "                        ffn_output)\n",
    "                k_ffn = layers.Multiply()([k_ffn, inputs[1]])\n",
    "                k_ffn = layers.LayerNormalization(\n",
    "                    epsilon=1e-6, name=\"Kffn_normalization\")(k_ffn)\n",
    "                k_ffn = layers.Multiply()([k_ffn, inputs[1]])\n",
    "                #         ffn_output= layers.Multiply()([ffn_output, inputs[1]])\n",
    "                ffn_output = layers.Dropout(DROPOUT)(ffn_output)\n",
    "\n",
    "                ### =======  Add and Norm ======= ###\n",
    "                layer_norm_output3 = layers.LayerNormalization(\n",
    "                    epsilon=1e-6,\n",
    "                    name=\"DFF_Norm\")(ffn_output + layer_norm_output2)\n",
    "\n",
    "                ### =======  Masked MultiHead Attention ======= ###\n",
    "                mha_output3, attention_weights = MultiHeadAttention(\n",
    "                    d_model=EMBED_DIM, num_heads=NUM_HEADS)(v=v_input,\n",
    "                                                            k=k_input,\n",
    "                                                            q=q_ffn,\n",
    "                                                            mask=None)\n",
    "                mha_masked3 = layers.Multiply(name=\"MHA_Masking_3\")(\n",
    "                    [mha_output3, inputs[1]])\n",
    "                mha_masked3 = layers.Dropout(DROPOUT)(mha_masked3)\n",
    "\n",
    "                ### =======  Add and Norm ======= ###\n",
    "                layer_norm_output4 = layers.LayerNormalization(\n",
    "                    epsilon=1e-6, name=\"MHA_Norm_3\")(mha_masked3 + mha_output3)\n",
    "\n",
    "                ### =======  Masked MultiHead Attention ======= ###\n",
    "                mha_output4, attention_weights = MultiHeadAttention(\n",
    "                    d_model=EMBED_DIM,\n",
    "                    num_heads=NUM_HEADS)(v=v_input,\n",
    "                                         k=k_input,\n",
    "                                         q=layer_norm_output4,\n",
    "                                         mask=None)\n",
    "                mha_output4 = layers.Multiply()([mha_output4, inputs[1]])\n",
    "                mha_output4 = layers.Dropout(DROPOUT)(mha_output4)\n",
    "\n",
    "                ### =======  Add and Norm ======= ###\n",
    "                layer_norm_output5 = layers.LayerNormalization(\n",
    "                    epsilon=1e-6,\n",
    "                    name=\"MHA_Norm_5\")(mha_output4 + layer_norm_output4)\n",
    "\n",
    "                ### =======  Feed Forward Network ======= ###\n",
    "                ffn2_output = layers.Dense(\n",
    "                    EMBED_DIM,\n",
    "                    activation='relu',\n",
    "                    name=\"DFF_4\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.001))(\n",
    "                        layer_norm_output5)\n",
    "                ffn2_output = layers.Dropout(DROPOUT)(\n",
    "                    ffn2_output)  # 0.2 * SET_SIZE_RATIO\n",
    "                ffn2_output = layers.Dense(\n",
    "                    EMBED_DIM,\n",
    "                    name=\"DFF_5\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.0001))(\n",
    "                        ffn2_output)\n",
    "                ffn2_output = layers.Dropout(DROPOUT)(ffn2_output)\n",
    "\n",
    "                ### =======  Add and Norm ======= ###\n",
    "                layer_norm_output6 = layers.LayerNormalization(\n",
    "                    epsilon=1e-6,\n",
    "                    name=\"DFF_Norm2\")(ffn2_output + layer_norm_output5)\n",
    "                layer_norm_output6 = layers.Multiply()(\n",
    "                    [layer_norm_output6, inputs[1]])\n",
    "                forLSTM = layers.Masking(name=\"DFF_Mask2\")(\n",
    "                    layer_norm_output6, )\n",
    "\n",
    "        ### =======  Bidirectional LSTM  ======= ###\n",
    "        LSTM3 = layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                EMBED_DIM // 2,  # EMBED_DIM//2,\n",
    "                dropout=DROPOUT,\n",
    "                recurrent_dropout=0,\n",
    "                activation='tanh',\n",
    "                use_bias=True,\n",
    "                unroll=False,\n",
    "                recurrent_activation='sigmoid'),\n",
    "            name=\"BLSTM\")(inputs=forLSTM)\n",
    "        LSTM3 = layers.LayerNormalization(epsilon=1e-6,\n",
    "                                          name=\"LSTM_Norm_3\")(LSTM3)\n",
    "\n",
    "        LSTM4 = layers.LSTM(\n",
    "            EMBED_DIM,  # EMBED_DIM//2,\n",
    "            dropout=DROPOUT,\n",
    "            recurrent_dropout=0,\n",
    "            activation='tanh',\n",
    "            use_bias=True,\n",
    "            unroll=False,\n",
    "            recurrent_activation='sigmoid')(inputs=inputs[0])\n",
    "        LSTM4 = layers.Dense(\n",
    "            HIDDEN_SIZE // 2,\n",
    "            activation='relu',\n",
    "            name=\"DFF_small_sets\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.0001))(LSTM4)\n",
    "        LSTM4 = layers.LayerNormalization(epsilon=1e-6,\n",
    "                                          name=\"LSTM_Norm_small\")(LSTM4)\n",
    "\n",
    "        ### =======  Feed Forward Network ======= ###\n",
    "        final_hidden = layers.Dense(\n",
    "            EMBED_DIM,\n",
    "            activation='relu',\n",
    "            name=\"DFF_last\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.0001))(LSTM3)\n",
    "        final_hidden = layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"DFF_normalize\")(final_hidden)\n",
    "        final_hidden = layers.Dropout(DROPOUT)(final_hidden)\n",
    "\n",
    "        ### =======  Define Multiple Outputs ======= ###\n",
    "        outputs_class = layers.Dense(num_classes,\n",
    "                                     activation='relu',\n",
    "                                     name=\"Classification\")(final_hidden)\n",
    "        outputs_regression = layers.Dense(1,\n",
    "                                          activation='relu',\n",
    "                                          name=\"Regression\")(final_hidden)\n",
    "\n",
    "        outputs_combined = layers.Concatenate()(\n",
    "            [outputs_class, outputs_regression, LSTM3])\n",
    "\n",
    "        if avg_len_dict[ess_set] < 200:\n",
    "            outputs_combined = layers.Concatenate()(\n",
    "                [outputs_class, outputs_regression])\n",
    "            outputs_combined = layers.Dense(\n",
    "                HIDDEN_SIZE, activation='relu',\n",
    "                name=\"Hidden_Class_small\")(outputs_combined)\n",
    "            outputs_combined = layers.Dropout(DROPOUT * 1.3)(outputs_combined)\n",
    "            outputs_combined = layers.Concatenate()([outputs_combined, LSTM3])\n",
    "            outputs_combined = layers.Dense(\n",
    "                HIDDEN_SIZE, activation='relu',\n",
    "                name=\"Hidden_Class_small2\")(outputs_combined)\n",
    "            outputs_combined = layers.Dropout(DROPOUT * 1.3)(outputs_combined)\n",
    "            outputs_combined = layers.Dense(\n",
    "                HIDDEN_SIZE // 2,\n",
    "                activation='relu',\n",
    "                name=\"Hidden_Class_small3\")(outputs_combined)\n",
    "            outputs_combined = layers.Dropout(DROPOUT * 1.3)(outputs_combined)\n",
    "\n",
    "        elif avg_len_dict[ess_set] < 400:\n",
    "            outputs_combined = layers.Concatenate()(\n",
    "                [outputs_class, outputs_regression, LSTM3])\n",
    "            outputs_combined = layers.Dense(\n",
    "                HIDDEN_SIZE // 2, activation='relu',\n",
    "                name=\"Hidden_Class_med1\")(outputs_combined)\n",
    "            outputs_combined = layers.Dropout(DROPOUT * 1.25)(outputs_combined)\n",
    "            outputs_combined = layers.Dense(\n",
    "                HIDDEN_SIZE // 4, activation='relu',\n",
    "                name=\"Hidden_Class_med2\")(outputs_combined)\n",
    "            outputs_combined = layers.Dropout(DROPOUT * 1.25)(outputs_combined)\n",
    "        else:\n",
    "            outputs_combined = layers.Concatenate()(\n",
    "                [outputs_class, outputs_regression, LSTM3])\n",
    "            outputs_combined = layers.Dropout(DROPOUT * 1.25)(outputs_combined)\n",
    "\n",
    "        outputs_combined_reg = layers.Dense(\n",
    "            1, activation='sigmoid', name=\"Combined_Regress\")(outputs_combined)\n",
    "        outputs_combined_class = layers.Dense(\n",
    "            HIDDEN_SIZE, activation='relu',\n",
    "            name=\"Final_hidden_Class\")(outputs_combined)\n",
    "\n",
    "        outputs_combined_class = layers.Dense(\n",
    "            num_classes, activation='softmax',\n",
    "            name=\"Combined_Class\")(outputs_combined_class)\n",
    "\n",
    "        ### =======  Output ======= ###\n",
    "        final_outputs = [outputs_combined_class, outputs_combined_reg]\n",
    "\n",
    "        ### =======  Define Model Parameters and Compile ======= ###\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=final_outputs)\n",
    "\n",
    "        loss = [\n",
    "            WeightedKappaLoss(num_classes=num_classes, label_smoothing=0),\n",
    "            tf.keras.losses.MeanSquaredError(),\n",
    "            WeightedKappaLoss(num_classes=num_classes, label_smoothing=0),\n",
    "        ]\n",
    "\n",
    "        learning_rate = LEARNING_RATE\n",
    "        learning_rate = CustomSchedule(d_model=EMBED_DIM,\n",
    "                                       warmup_steps=750,\n",
    "                                       is_sine=False)\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['acc', 'mse'],\n",
    "            loss_weights=[LOSS_WEIGHT, num_classes * (1 - LOSS_WEIGHT)],\n",
    "        )\n",
    "\n",
    "        ### =======  Train Model ======= ###\n",
    "\n",
    "        callback = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_Combined_Class_loss',\n",
    "                                             patience=PATIENCE,\n",
    "                                             min_delta=0.01,\n",
    "                                             restore_best_weights=True),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_Combined_Regress_loss',\n",
    "                patience=PATIENCE * 6,\n",
    "                min_delta=0.0001,\n",
    "                restore_best_weights=True),\n",
    "        ]\n",
    "        history = model.fit(\n",
    "            [Xn, Mn, sMn],\n",
    "            [yn, yn_regression],\n",
    "            validation_data=([Xv, Mv, sMv], [yv, yv_regression]),\n",
    "            batch_size=512,  # BATCH_SIZES,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[callback])\n",
    "\n",
    "        ### =======  Record History and Summaries ======= ###\n",
    "        ess_history.append(history)\n",
    "        model.summary()\n",
    "\n",
    "        print(\"BATCH_SIZES = {}\".format(BATCH_SIZES))\n",
    "        print(\"HIDDEN_SIZE = {}\".format(HIDDEN_SIZE))\n",
    "        print(\"TOTAL EPOCHS =  {}\".format(EPOCHS))\n",
    "        print(\"PATIENCE = {}\".format(PATIENCE))\n",
    "        print(\"DROPOUT = {}\".format(DROPOUT))\n",
    "        print(\"LEARNING_RATE =  {}\".format(LEARNING_RATE))\n",
    "        print(\"percent classification = {} \".format(LOSS_WEIGHT))\n",
    "        print(\"NUM_HEADS = {}\".format(NUM_HEADS))\n",
    "\n",
    "        ### =======  Evaluate on Held-out Test Set ======= ###\n",
    "        y_pred = model.predict([Xt, Mt, sMt])\n",
    "        y_pred_class = y_pred[0]\n",
    "        y_pred_class_c = np.argmax(y_pred[0], axis=1)\n",
    "        y_pred_regression = y_pred[1]\n",
    "        y_pred_regression = np.round(y_pred_regression * num_classes)\n",
    "        y_pred_class = np.argmax(y_pred_class, axis=1)\n",
    "\n",
    "        print(confusion_matrix(yt, y_pred_class))\n",
    "        print(\"\\n--------Fold {} Classification KAPPA--------\\n\".format(count))\n",
    "        class_result = cohen_kappa_score(yt, y_pred_class, weights='quadratic')\n",
    "        print(class_result)\n",
    "\n",
    "        print(confusion_matrix(yt, y_pred_regression))\n",
    "        print(\"\\n--------Fold {} Regression KAPPA--------\\n\".format(count))\n",
    "        regress_result = cohen_kappa_score(yt,\n",
    "                                           y_pred_regression,\n",
    "                                           weights='quadratic')\n",
    "        print(regress_result)\n",
    "        y_comb = np.round(\n",
    "            np.add((np.argmax(y_pred[0], axis=1) * LOSS_WEIGHT),\n",
    "                   (y_pred[1] * num_classes * (1 - LOSS_WEIGHT))[:, 0]))\n",
    "\n",
    "        print(confusion_matrix(yt, y_comb))\n",
    "        print(\"\\n--------Fold {} Combined KAPPA--------\\n\".format(count))\n",
    "        comb_result = cohen_kappa_score(yt, y_comb, weights='quadratic')\n",
    "        print(comb_result)\n",
    "\n",
    "        class_results.append(class_result)\n",
    "        regress_results.append(regress_result)\n",
    "        comb_results.append(comb_result)\n",
    "        if count == FOLDS:\n",
    "            print(\"{}__FOLD_CLASS_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(class_results)))\n",
    "            print(\"{}__FOLD_REGRESS_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(regress_results)))\n",
    "            print(\"{}__FOLD_COMB_KAPPA___SCORE___MAX: {}\".format(\n",
    "                FOLDS, np.max(comb_results)))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    histories_mha_combined_objective_final4.append(ess_history)\n",
    "    result_history.append(results)\n",
    "    f, ax = plt.subplots(nrows=1, ncols=FOLDS, figsize=[15, 3], sharey='row')\n",
    "    for fd in range(FOLDS):\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['Combined_Regress_loss'])\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['val_Combined_Regress_loss'])\n",
    "    plt.title('model regress loss essay {}'.format(ess_set))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        'mha2_combined_regress_features_8_6_capacity_essay_{}.png'.format(\n",
    "            ess_set))\n",
    "\n",
    "    f, ax = plt.subplots(nrows=1, ncols=FOLDS, figsize=[15, 3], sharey='row')\n",
    "    for fd in range(FOLDS):\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['Combined_Class_loss'])\n",
    "        ax[fd].plot(histories_mha_combined_objective_final4[essay_set_counter]\n",
    "                    [fd].history['val_Combined_Class_loss'])\n",
    "    plt.title('model class loss essay {}'.format(ess_set))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        'mha2_combined_class_features_8_6_capacity_essay_{}.png'.format(\n",
    "            ess_set))\n",
    "    essay_set_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
